{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you already ran the \"preparing data\", you can skip this section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.import_data as import_data\n",
    "import modules.prepare_data as prepare_data\n",
    "import modules.natural_language_treatment as nlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTranscripts = import_data.import_transcripts(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transcripts: 81822\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serie</th>\n",
       "      <th>season</th>\n",
       "      <th>nb_episode</th>\n",
       "      <th>name_episode</th>\n",
       "      <th>transcript</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot_(1)</td>\n",
       "      <td>1\\n00:02:50,904 --&gt; 00:02:52,929\\nHelp me!\\n\\n...</td>\n",
       "      <td>data/transcripts/1___Lost/01/01__Pilot_(1).txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>2</td>\n",
       "      <td>Pilot_(2)</td>\n",
       "      <td>1\\n00:00:18,752 --&gt; 00:00:20,310\\nAnything?\\n\\...</td>\n",
       "      <td>data/transcripts/1___Lost/01/02__Pilot_(2).txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>3</td>\n",
       "      <td>Tabula_rasa</td>\n",
       "      <td>1\\n00:00:02,002 --&gt; 00:00:03,663\\n&lt;i&gt;Previousl...</td>\n",
       "      <td>data/transcripts/1___Lost/01/03__Tabula_rasa.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>4</td>\n",
       "      <td>Walkabout</td>\n",
       "      <td>1\\n00:00:36,469 --&gt; 00:00:38,903\\nHelp me!\\n\\n...</td>\n",
       "      <td>data/transcripts/1___Lost/01/04__Walkabout.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>5</td>\n",
       "      <td>White_rabbit</td>\n",
       "      <td>1\\n00:00:10,176 --&gt; 00:00:11,666\\nStay down.\\n...</td>\n",
       "      <td>data/transcripts/1___Lost/01/05__White_rabbit.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  serie season  nb_episode  name_episode  \\\n",
       "0  Lost     01           1     Pilot_(1)   \n",
       "1  Lost     01           2     Pilot_(2)   \n",
       "2  Lost     01           3   Tabula_rasa   \n",
       "3  Lost     01           4     Walkabout   \n",
       "4  Lost     01           5  White_rabbit   \n",
       "\n",
       "                                          transcript  \\\n",
       "0  1\\n00:02:50,904 --> 00:02:52,929\\nHelp me!\\n\\n...   \n",
       "1  1\\n00:00:18,752 --> 00:00:20,310\\nAnything?\\n\\...   \n",
       "2  1\\n00:00:02,002 --> 00:00:03,663\\n<i>Previousl...   \n",
       "3  1\\n00:00:36,469 --> 00:00:38,903\\nHelp me!\\n\\n...   \n",
       "4  1\\n00:00:10,176 --> 00:00:11,666\\nStay down.\\n...   \n",
       "\n",
       "                                                path  \n",
       "0     data/transcripts/1___Lost/01/01__Pilot_(1).txt  \n",
       "1     data/transcripts/1___Lost/01/02__Pilot_(2).txt  \n",
       "2   data/transcripts/1___Lost/01/03__Tabula_rasa.txt  \n",
       "3     data/transcripts/1___Lost/01/04__Walkabout.txt  \n",
       "4  data/transcripts/1___Lost/01/05__White_rabbit.txt  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of transcripts:\", len(dfTranscripts))\n",
    "dfTranscripts.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing part of transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\n00:02:50,904 --> 00:02:52,929\\nHelp me!\\n\\n2\\n00:02:53,006 --> 00:02:55,406\\nSomebody, help me out.\\n\\n3\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTranscripts.iloc[0][\"transcript\"][:100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying function to remove timecodes and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 291 timecodes and 11017 numbers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nHelp me!\\n\\n\\n\\nSomebody, help me out.\\n\\n\\n\\nSomebody, help me over here!\\n\\n\\n\\nSomebody, help me out!\\n\\n\\n\\nOh'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transcript, count_removed_timecodes, count_removed_numbers = prepare_data.remove_timecodes_and_number(dfTranscripts.iloc[0][\"transcript\"])\n",
    "print(\"Removed\", count_removed_timecodes, \"timecodes and\", count_removed_numbers, \"numbers\")\n",
    "new_transcript[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Help me!Somebody, help me out.Somebody, help me over here!Somebody, help me out!Oh, my God!Walt! Walt!- Stay away from the gas! Stay there!Help! Help!Somebody, help me!Oh, my leg!Hey, get over here.Give me a hand.You, come on!Come over here! Give me a hand!On the count of three.One, two, three!Help!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transcript,count_removed_newline = prepare_data.remove_newline(new_transcript)\n",
    "new_transcript[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Help me Somebody  help me out Somebody  help me over here Somebody  help me out Oh  my God Walt  Walt   Stay away from the gas  Stay there Help  Help Somebody  help me Oh  my leg Hey  get over here Give me a hand You  come on Come over here  Give me a hand On the count of three One  two  three Help '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transcript, count_removed_punctuation = prepare_data.remove_punctuation(new_transcript)\n",
    "new_transcript[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 313 spaces\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Help me Somebody help me out Somebody help me over here Somebody help me out Oh my God Walt Walt Stay away from the gas Stay there Help Help Somebody help me Oh my leg Hey get over here Give me a hand You come on Come over here Give me a hand On the count of three One two three Help Please help me H'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transcript, count_removed_multiple_spaces = prepare_data.remove_multiple_spaces(new_transcript)\n",
    "print(\"Removed\", count_removed_multiple_spaces, \"spaces\")\n",
    "new_transcript[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transcript = prepare_data.convert_to_lowercase(new_transcript)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare all the data for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serie</th>\n",
       "      <th>season</th>\n",
       "      <th>nb_episode</th>\n",
       "      <th>name_episode</th>\n",
       "      <th>transcript</th>\n",
       "      <th>path</th>\n",
       "      <th>transcripts_prepared_for_tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot_(1)</td>\n",
       "      <td>1\\n00:02:50,904 --&gt; 00:02:52,929\\nHelp me!\\n\\n...</td>\n",
       "      <td>data/transcripts/1___Lost/01/01__Pilot_(1).txt</td>\n",
       "      <td>help me somebody help me out somebody help me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>2</td>\n",
       "      <td>Pilot_(2)</td>\n",
       "      <td>1\\n00:00:18,752 --&gt; 00:00:20,310\\nAnything?\\n\\...</td>\n",
       "      <td>data/transcripts/1___Lost/01/02__Pilot_(2).txt</td>\n",
       "      <td>anything you keep asking if there s anything p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>3</td>\n",
       "      <td>Tabula_rasa</td>\n",
       "      <td>1\\n00:00:02,002 --&gt; 00:00:03,663\\n&lt;i&gt;Previousl...</td>\n",
       "      <td>data/transcripts/1___Lost/01/03__Tabula_rasa.txt</td>\n",
       "      <td>i previously on lost i do you think he s gonn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>4</td>\n",
       "      <td>Walkabout</td>\n",
       "      <td>1\\n00:00:36,469 --&gt; 00:00:38,903\\nHelp me!\\n\\n...</td>\n",
       "      <td>data/transcripts/1___Lost/01/04__Walkabout.txt</td>\n",
       "      <td>help me walt you gottakeep that dog quiet i do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>5</td>\n",
       "      <td>White_rabbit</td>\n",
       "      <td>1\\n00:00:10,176 --&gt; 00:00:11,666\\nStay down.\\n...</td>\n",
       "      <td>data/transcripts/1___Lost/01/05__White_rabbit.txt</td>\n",
       "      <td>stay down your choice man walk away now you wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81817</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>3</td>\n",
       "      <td>Rome</td>\n",
       "      <td>ï»¿1\\n00:00:00,033 --&gt; 00:00:02,601\\n      Hel...</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/03__...</td>\n",
       "      <td>help everyone explore new worlds and ideas su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81818</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>4</td>\n",
       "      <td>Hunger</td>\n",
       "      <td>ï»¿1\\n00:00:00,033 --&gt; 00:00:02,601\\n      Hel...</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/04__...</td>\n",
       "      <td>help everyone explore new worlds and ideas su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81819</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>5</td>\n",
       "      <td>Past</td>\n",
       "      <td>1\\n00:00:00,033 --&gt; 00:00:02,568\\n      Help e...</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/05__...</td>\n",
       "      <td>help everyone explore new worlds and ideas su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81820</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>6</td>\n",
       "      <td>Hustle</td>\n",
       "      <td>ï»¿1\\n00:00:00,033 --&gt; 00:00:02,568\\n      Hel...</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/06__...</td>\n",
       "      <td>help everyone explore new worlds and ideas su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81821</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>7</td>\n",
       "      <td>Napkin</td>\n",
       "      <td>1\\n00:00:01,033 --&gt; 00:00:03,568\\n      Help e...</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/07__...</td>\n",
       "      <td>help everyone explore new worlds and ideas su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81822 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                serie season  nb_episode  name_episode  \\\n",
       "0                Lost     01           1     Pilot_(1)   \n",
       "1                Lost     01           2     Pilot_(2)   \n",
       "2                Lost     01           3   Tabula_rasa   \n",
       "3                Lost     01           4     Walkabout   \n",
       "4                Lost     01           5  White_rabbit   \n",
       "...               ...    ...         ...           ...   \n",
       "81817  Mind_of_a_Chef     04           3          Rome   \n",
       "81818  Mind_of_a_Chef     04           4        Hunger   \n",
       "81819  Mind_of_a_Chef     04           5          Past   \n",
       "81820  Mind_of_a_Chef     04           6        Hustle   \n",
       "81821  Mind_of_a_Chef     04           7        Napkin   \n",
       "\n",
       "                                              transcript  \\\n",
       "0      1\\n00:02:50,904 --> 00:02:52,929\\nHelp me!\\n\\n...   \n",
       "1      1\\n00:00:18,752 --> 00:00:20,310\\nAnything?\\n\\...   \n",
       "2      1\\n00:00:02,002 --> 00:00:03,663\\n<i>Previousl...   \n",
       "3      1\\n00:00:36,469 --> 00:00:38,903\\nHelp me!\\n\\n...   \n",
       "4      1\\n00:00:10,176 --> 00:00:11,666\\nStay down.\\n...   \n",
       "...                                                  ...   \n",
       "81817  ï»¿1\\n00:00:00,033 --> 00:00:02,601\\n      Hel...   \n",
       "81818  ï»¿1\\n00:00:00,033 --> 00:00:02,601\\n      Hel...   \n",
       "81819  1\\n00:00:00,033 --> 00:00:02,568\\n      Help e...   \n",
       "81820  ï»¿1\\n00:00:00,033 --> 00:00:02,568\\n      Hel...   \n",
       "81821  1\\n00:00:01,033 --> 00:00:03,568\\n      Help e...   \n",
       "\n",
       "                                                    path  \\\n",
       "0         data/transcripts/1___Lost/01/01__Pilot_(1).txt   \n",
       "1         data/transcripts/1___Lost/01/02__Pilot_(2).txt   \n",
       "2       data/transcripts/1___Lost/01/03__Tabula_rasa.txt   \n",
       "3         data/transcripts/1___Lost/01/04__Walkabout.txt   \n",
       "4      data/transcripts/1___Lost/01/05__White_rabbit.txt   \n",
       "...                                                  ...   \n",
       "81817  data/transcripts/5480___Mind_of_a_Chef/04/03__...   \n",
       "81818  data/transcripts/5480___Mind_of_a_Chef/04/04__...   \n",
       "81819  data/transcripts/5480___Mind_of_a_Chef/04/05__...   \n",
       "81820  data/transcripts/5480___Mind_of_a_Chef/04/06__...   \n",
       "81821  data/transcripts/5480___Mind_of_a_Chef/04/07__...   \n",
       "\n",
       "                   transcripts_prepared_for_tokenization  \n",
       "0      help me somebody help me out somebody help me ...  \n",
       "1      anything you keep asking if there s anything p...  \n",
       "2       i previously on lost i do you think he s gonn...  \n",
       "3      help me walt you gottakeep that dog quiet i do...  \n",
       "4      stay down your choice man walk away now you wo...  \n",
       "...                                                  ...  \n",
       "81817   help everyone explore new worlds and ideas su...  \n",
       "81818   help everyone explore new worlds and ideas su...  \n",
       "81819   help everyone explore new worlds and ideas su...  \n",
       "81820   help everyone explore new worlds and ideas su...  \n",
       "81821   help everyone explore new worlds and ideas su...  \n",
       "\n",
       "[81822 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTranscriptsTokenization =  prepare_data.prepare_data_in_dataframe(dfTranscripts)\n",
    "dfTranscriptsTokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTranscriptsTokenization = dfTranscriptsTokenization.drop('transcript', axis=1)\n",
    "dfTranscriptsTokenization.to_csv(\"data/pandas_export/prepared_for_tokenization.csv\", sep=\";\", escapechar=\"\\\\\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTranscriptsTokenization = pd.read_csv('data/pandas_export/prepared_for_tokenization.csv', sep=\";\", escapechar=\"\\\\\")\n",
    "dfTranscriptsTokenization = dfTranscriptsTokenization[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>serie</th>\n",
       "      <th>season</th>\n",
       "      <th>nb_episode</th>\n",
       "      <th>name_episode</th>\n",
       "      <th>path</th>\n",
       "      <th>transcripts_prepared_for_tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Lost</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot_(1)</td>\n",
       "      <td>data/transcripts/1___Lost/01/01__Pilot_(1).txt</td>\n",
       "      <td>help me somebody help me out somebody help me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lost</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Pilot_(2)</td>\n",
       "      <td>data/transcripts/1___Lost/01/02__Pilot_(2).txt</td>\n",
       "      <td>anything you keep asking if there s anything p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lost</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Tabula_rasa</td>\n",
       "      <td>data/transcripts/1___Lost/01/03__Tabula_rasa.txt</td>\n",
       "      <td>i previously on lost i do you think he s gonn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Lost</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Walkabout</td>\n",
       "      <td>data/transcripts/1___Lost/01/04__Walkabout.txt</td>\n",
       "      <td>help me walt you gottakeep that dog quiet i do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Lost</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>White_rabbit</td>\n",
       "      <td>data/transcripts/1___Lost/01/05__White_rabbit.txt</td>\n",
       "      <td>stay down your choice man walk away now you wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>Lost</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>He_s_Our_You</td>\n",
       "      <td>data/transcripts/1___Lost/05/10__He_s_Our_You.txt</td>\n",
       "      <td>previously on i lost i are you one of them one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>Lost</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>Whatever_Happened</td>\n",
       "      <td>data/transcripts/1___Lost/05/11__Whatever_Happ...</td>\n",
       "      <td>previously on lost this is your daughter her n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>Lost</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>Dead_is_Dead</td>\n",
       "      <td>data/transcripts/1___Lost/05/12__Dead_is_Dead.txt</td>\n",
       "      <td>previously on lost is that benjamin linus we n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>Lost</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>Some_Like_It_Hoth</td>\n",
       "      <td>data/transcripts/1___Lost/05/13__Some_Like_It_...</td>\n",
       "      <td>comes fully equipped i m telling you it s a st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>Lost</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>The_Variable</td>\n",
       "      <td>data/transcripts/1___Lost/05/14__The_Variable.txt</td>\n",
       "      <td>on lost no no desmond i need you to go backto ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 serie  season  nb_episode       name_episode  \\\n",
       "0            0  Lost       1           1          Pilot_(1)   \n",
       "1            1  Lost       1           2          Pilot_(2)   \n",
       "2            2  Lost       1           3        Tabula_rasa   \n",
       "3            3  Lost       1           4          Walkabout   \n",
       "4            4  Lost       1           5       White_rabbit   \n",
       "..         ...   ...     ...         ...                ...   \n",
       "95          95  Lost       5          10       He_s_Our_You   \n",
       "96          96  Lost       5          11  Whatever_Happened   \n",
       "97          97  Lost       5          12       Dead_is_Dead   \n",
       "98          98  Lost       5          13  Some_Like_It_Hoth   \n",
       "99          99  Lost       5          14       The_Variable   \n",
       "\n",
       "                                                 path  \\\n",
       "0      data/transcripts/1___Lost/01/01__Pilot_(1).txt   \n",
       "1      data/transcripts/1___Lost/01/02__Pilot_(2).txt   \n",
       "2    data/transcripts/1___Lost/01/03__Tabula_rasa.txt   \n",
       "3      data/transcripts/1___Lost/01/04__Walkabout.txt   \n",
       "4   data/transcripts/1___Lost/01/05__White_rabbit.txt   \n",
       "..                                                ...   \n",
       "95  data/transcripts/1___Lost/05/10__He_s_Our_You.txt   \n",
       "96  data/transcripts/1___Lost/05/11__Whatever_Happ...   \n",
       "97  data/transcripts/1___Lost/05/12__Dead_is_Dead.txt   \n",
       "98  data/transcripts/1___Lost/05/13__Some_Like_It_...   \n",
       "99  data/transcripts/1___Lost/05/14__The_Variable.txt   \n",
       "\n",
       "                transcripts_prepared_for_tokenization  \n",
       "0   help me somebody help me out somebody help me ...  \n",
       "1   anything you keep asking if there s anything p...  \n",
       "2    i previously on lost i do you think he s gonn...  \n",
       "3   help me walt you gottakeep that dog quiet i do...  \n",
       "4   stay down your choice man walk away now you wo...  \n",
       "..                                                ...  \n",
       "95  previously on i lost i are you one of them one...  \n",
       "96  previously on lost this is your daughter her n...  \n",
       "97  previously on lost is that benjamin linus we n...  \n",
       "98  comes fully equipped i m telling you it s a st...  \n",
       "99  on lost no no desmond i need you to go backto ...  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTranscriptsTokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run if you already installed nltk popular wordlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>serie</th>\n",
       "      <th>season</th>\n",
       "      <th>nb_episode</th>\n",
       "      <th>name_episode</th>\n",
       "      <th>path</th>\n",
       "      <th>transcript_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Lost</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot_(1)</td>\n",
       "      <td>data/transcripts/1___Lost/01/01__Pilot_(1).txt</td>\n",
       "      <td>[help, me, somebody, help, me, out, somebody, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lost</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Pilot_(2)</td>\n",
       "      <td>data/transcripts/1___Lost/01/02__Pilot_(2).txt</td>\n",
       "      <td>[anything, you, keep, asking, if, there, s, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lost</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Tabula_rasa</td>\n",
       "      <td>data/transcripts/1___Lost/01/03__Tabula_rasa.txt</td>\n",
       "      <td>[i, previously, on, lost, i, do, you, think, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Lost</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Walkabout</td>\n",
       "      <td>data/transcripts/1___Lost/01/04__Walkabout.txt</td>\n",
       "      <td>[help, me, walt, you, gottakeep, that, dog, qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Lost</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>White_rabbit</td>\n",
       "      <td>data/transcripts/1___Lost/01/05__White_rabbit.txt</td>\n",
       "      <td>[stay, down, your, choice, man, walk, away, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>295</td>\n",
       "      <td>Prison_Break</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Safe_and_Sound</td>\n",
       "      <td>data/transcripts/4___Prison_Break/04/05__Safe_...</td>\n",
       "      <td>[previously, on, prison, break, it, was, a, me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>296</td>\n",
       "      <td>Prison_Break</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Blow_Out</td>\n",
       "      <td>data/transcripts/4___Prison_Break/04/06__Blow_...</td>\n",
       "      <td>[previously, on, prison, breaksomeone, at, hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>297</td>\n",
       "      <td>Prison_Break</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Five_the_Hard_Way</td>\n",
       "      <td>data/transcripts/4___Prison_Break/04/07__Five_...</td>\n",
       "      <td>[previously, on, prison, break, these, are, gp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>Prison_Break</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>The_Price</td>\n",
       "      <td>data/transcripts/4___Prison_Break/04/08__The_P...</td>\n",
       "      <td>[previously, on, prison, break, if, scylla, wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>299</td>\n",
       "      <td>Prison_Break</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Greatness_Achieved</td>\n",
       "      <td>data/transcripts/4___Prison_Break/04/09__Great...</td>\n",
       "      <td>[previously, on, prison, break, what, do, you,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0         serie  season  nb_episode        name_episode  \\\n",
       "0             0          Lost       1           1           Pilot_(1)   \n",
       "1             1          Lost       1           2           Pilot_(2)   \n",
       "2             2          Lost       1           3         Tabula_rasa   \n",
       "3             3          Lost       1           4           Walkabout   \n",
       "4             4          Lost       1           5        White_rabbit   \n",
       "..          ...           ...     ...         ...                 ...   \n",
       "295         295  Prison_Break       4           5      Safe_and_Sound   \n",
       "296         296  Prison_Break       4           6            Blow_Out   \n",
       "297         297  Prison_Break       4           7   Five_the_Hard_Way   \n",
       "298         298  Prison_Break       4           8           The_Price   \n",
       "299         299  Prison_Break       4           9  Greatness_Achieved   \n",
       "\n",
       "                                                  path  \\\n",
       "0       data/transcripts/1___Lost/01/01__Pilot_(1).txt   \n",
       "1       data/transcripts/1___Lost/01/02__Pilot_(2).txt   \n",
       "2     data/transcripts/1___Lost/01/03__Tabula_rasa.txt   \n",
       "3       data/transcripts/1___Lost/01/04__Walkabout.txt   \n",
       "4    data/transcripts/1___Lost/01/05__White_rabbit.txt   \n",
       "..                                                 ...   \n",
       "295  data/transcripts/4___Prison_Break/04/05__Safe_...   \n",
       "296  data/transcripts/4___Prison_Break/04/06__Blow_...   \n",
       "297  data/transcripts/4___Prison_Break/04/07__Five_...   \n",
       "298  data/transcripts/4___Prison_Break/04/08__The_P...   \n",
       "299  data/transcripts/4___Prison_Break/04/09__Great...   \n",
       "\n",
       "                                  transcript_tokenized  \n",
       "0    [help, me, somebody, help, me, out, somebody, ...  \n",
       "1    [anything, you, keep, asking, if, there, s, an...  \n",
       "2    [i, previously, on, lost, i, do, you, think, h...  \n",
       "3    [help, me, walt, you, gottakeep, that, dog, qu...  \n",
       "4    [stay, down, your, choice, man, walk, away, no...  \n",
       "..                                                 ...  \n",
       "295  [previously, on, prison, break, it, was, a, me...  \n",
       "296  [previously, on, prison, breaksomeone, at, hom...  \n",
       "297  [previously, on, prison, break, these, are, gp...  \n",
       "298  [previously, on, prison, break, if, scylla, wa...  \n",
       "299  [previously, on, prison, break, what, do, you,...  \n",
       "\n",
       "[300 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTranscriptsTokenized = nlt.tokenization(dfTranscriptsTokenization)\n",
    "dfTranscriptsTokenized = dfTranscriptsTokenized.drop('transcripts_prepared_for_tokenization', axis=1)\n",
    "dfTranscriptsTokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aid',\n",
       " 'Maine',\n",
       " 'person',\n",
       " 'aid',\n",
       " 'Maine',\n",
       " 'out',\n",
       " 'person',\n",
       " 'aid',\n",
       " 'Maine',\n",
       " 'over',\n",
       " 'here',\n",
       " 'person',\n",
       " 'aid',\n",
       " 'Maine',\n",
       " 'out',\n",
       " 'Ohio',\n",
       " 'my',\n",
       " 'God',\n",
       " 'walt',\n",
       " 'walt',\n",
       " 'stay',\n",
       " 'away',\n",
       " 'from',\n",
       " 'the',\n",
       " 'gas',\n",
       " 'stay',\n",
       " 'there',\n",
       " 'aid',\n",
       " 'aid',\n",
       " 'person',\n",
       " 'aid',\n",
       " 'Maine',\n",
       " 'Ohio',\n",
       " 'my',\n",
       " 'leg',\n",
       " 'hey',\n",
       " 'get',\n",
       " 'over',\n",
       " 'here',\n",
       " 'give',\n",
       " 'Maine',\n",
       " 'angstrom',\n",
       " 'hand',\n",
       " 'you',\n",
       " 'semen',\n",
       " 'on',\n",
       " 'semen',\n",
       " 'over',\n",
       " 'here',\n",
       " 'give',\n",
       " 'Maine',\n",
       " 'angstrom',\n",
       " 'hand',\n",
       " 'on',\n",
       " 'the',\n",
       " 'count',\n",
       " 'of',\n",
       " 'three',\n",
       " 'one',\n",
       " 'two',\n",
       " 'three',\n",
       " 'aid',\n",
       " 'please',\n",
       " 'aid',\n",
       " 'Maine',\n",
       " 'aid',\n",
       " 'Maine',\n",
       " 'please',\n",
       " 'aid',\n",
       " 'Maine',\n",
       " 'get',\n",
       " 'him',\n",
       " 'out',\n",
       " 'of',\n",
       " 'here',\n",
       " 'get',\n",
       " 'him',\n",
       " 'away',\n",
       " 'from',\n",
       " 'the',\n",
       " 'engine',\n",
       " 'get',\n",
       " 'him',\n",
       " 'out',\n",
       " 'of',\n",
       " 'here',\n",
       " 'aid',\n",
       " 'Maine',\n",
       " 'please',\n",
       " 'aid',\n",
       " 'Maine',\n",
       " 'iodine',\n",
       " 'meter',\n",
       " 'have',\n",
       " 'contraction',\n",
       " 'how',\n",
       " 'many',\n",
       " 'month',\n",
       " 'pregnant',\n",
       " 'be',\n",
       " 'you',\n",
       " 'lone',\n",
       " 'eight',\n",
       " 'month',\n",
       " 'how',\n",
       " 'Army_for_the_Liberation_of_Rwanda',\n",
       " 'apart',\n",
       " 'be',\n",
       " 'they',\n",
       " 'come',\n",
       " 'iodine',\n",
       " 'Don',\n",
       " 'thymine',\n",
       " 'know',\n",
       " 'angstrom',\n",
       " 'few',\n",
       " 'just',\n",
       " 'happen',\n",
       " 'hey',\n",
       " 'get',\n",
       " 'away',\n",
       " 'from',\n",
       " 'there',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'Maine',\n",
       " 'expression',\n",
       " 'astatine',\n",
       " 'Maine',\n",
       " 'you',\n",
       " 'rhenium',\n",
       " 'gon',\n",
       " 'sodium',\n",
       " 'beryllium',\n",
       " 'O.K.',\n",
       " 'bash',\n",
       " 'you',\n",
       " 'understand',\n",
       " 'Maine',\n",
       " 'merely',\n",
       " 'you',\n",
       " 'rich_person',\n",
       " 'to',\n",
       " 'sit',\n",
       " 'absolutely',\n",
       " 'still',\n",
       " 'hey',\n",
       " 'you',\n",
       " 'semen',\n",
       " 'here',\n",
       " 'iodine',\n",
       " 'need',\n",
       " 'you',\n",
       " 'to',\n",
       " 'get',\n",
       " 'this',\n",
       " 'woman',\n",
       " 'awayfrom',\n",
       " 'these',\n",
       " 'fume',\n",
       " 'return',\n",
       " 'her',\n",
       " 'there',\n",
       " 'stay',\n",
       " 'with',\n",
       " 'her',\n",
       " 'if',\n",
       " 'her',\n",
       " 'contraction',\n",
       " 'occurcloser',\n",
       " 'than',\n",
       " 'three',\n",
       " 'minute',\n",
       " 'call',\n",
       " 'Maine',\n",
       " 'you',\n",
       " 'get',\n",
       " 'tantalum',\n",
       " 'beryllium',\n",
       " 'kid',\n",
       " 'Maine',\n",
       " 'iodine',\n",
       " 'll',\n",
       " 'beryllium',\n",
       " 'right',\n",
       " 'back',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'hey',\n",
       " 'what',\n",
       " 'second',\n",
       " 'your',\n",
       " 'name',\n",
       " 'jack',\n",
       " 'stop',\n",
       " 'her',\n",
       " 'head',\n",
       " 'second',\n",
       " 'not',\n",
       " 'tilt',\n",
       " 'back',\n",
       " 'enough',\n",
       " 'you',\n",
       " 'rhenium',\n",
       " 'blow',\n",
       " 'into',\n",
       " 'her',\n",
       " 'stomach',\n",
       " 'you',\n",
       " 'certain',\n",
       " 'that',\n",
       " 'second',\n",
       " 'precisely',\n",
       " 'what',\n",
       " 'iodine',\n",
       " 'wasdoing',\n",
       " 'iodine',\n",
       " 'meter',\n",
       " 'angstrom',\n",
       " 'lifeguard',\n",
       " 'iodine',\n",
       " 'meter',\n",
       " 'license',\n",
       " 'you',\n",
       " 'need',\n",
       " 'to',\n",
       " 'think',\n",
       " 'aboutgiving',\n",
       " 'that',\n",
       " 'license',\n",
       " 'back',\n",
       " 'possibly',\n",
       " 'we',\n",
       " 'should',\n",
       " 'bash',\n",
       " 'one',\n",
       " 'of',\n",
       " 'those',\n",
       " 'holethings',\n",
       " 'stick',\n",
       " 'the',\n",
       " 'pen',\n",
       " 'inch',\n",
       " 'the',\n",
       " 'throat',\n",
       " 'yea',\n",
       " 'good',\n",
       " 'idea',\n",
       " 'you',\n",
       " 'go',\n",
       " 'get',\n",
       " 'Maine',\n",
       " 'angstrom',\n",
       " 'pen',\n",
       " 'do',\n",
       " 'anyone',\n",
       " 'rich_person',\n",
       " 'any',\n",
       " 'pen',\n",
       " 'bash',\n",
       " 'you',\n",
       " 'rich_person',\n",
       " 'angstrom',\n",
       " 'pen',\n",
       " 'do',\n",
       " 'anybody',\n",
       " 'rich_person',\n",
       " 'angstrom',\n",
       " 'pen',\n",
       " 'semen',\n",
       " 'on',\n",
       " 'semen',\n",
       " 'on',\n",
       " 'semen',\n",
       " 'on',\n",
       " 'semen',\n",
       " 'on',\n",
       " 'large',\n",
       " 'deep',\n",
       " 'breath',\n",
       " 'large',\n",
       " 'breath',\n",
       " 'move',\n",
       " 'move',\n",
       " 'move',\n",
       " 'get',\n",
       " 'her',\n",
       " 'up',\n",
       " 'get',\n",
       " 'her',\n",
       " 'out',\n",
       " 'of',\n",
       " 'there',\n",
       " 'you',\n",
       " 'O.K.',\n",
       " 'yea',\n",
       " 'yea',\n",
       " 'you',\n",
       " 'stay',\n",
       " 'with',\n",
       " 'her',\n",
       " 'fellow',\n",
       " 'iodine',\n",
       " 'meter',\n",
       " 'not',\n",
       " 'go',\n",
       " 'anywhere',\n",
       " 'iodine',\n",
       " 'didn',\n",
       " 'thymine',\n",
       " 'knowwhich',\n",
       " 'one',\n",
       " 'would',\n",
       " 'work',\n",
       " 'best',\n",
       " 'they',\n",
       " 'rhenium',\n",
       " 'all',\n",
       " 'good',\n",
       " 'thank',\n",
       " 'excuse',\n",
       " 'Maine',\n",
       " 'do',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'use',\n",
       " 'angstrom',\n",
       " 'acerate_leaf',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'spot',\n",
       " 'angstrom',\n",
       " 'pair',\n",
       " 'of',\n",
       " 'jean',\n",
       " 'liter',\n",
       " 'uh',\n",
       " 'iodine',\n",
       " 'make',\n",
       " 'the',\n",
       " 'drape',\n",
       " 'inch',\n",
       " 'my',\n",
       " 'apartment',\n",
       " 'that',\n",
       " 'second',\n",
       " 'antic',\n",
       " 'listen',\n",
       " 'bash',\n",
       " 'you',\n",
       " 'rich_person',\n",
       " 'angstrom',\n",
       " 'second',\n",
       " 'iodine',\n",
       " 'could',\n",
       " 'use',\n",
       " 'angstrom',\n",
       " 'little',\n",
       " 'aid',\n",
       " 'here',\n",
       " 'aid',\n",
       " 'with',\n",
       " 'what',\n",
       " 'with',\n",
       " 'this',\n",
       " 'iodine',\n",
       " 'vitamin_D',\n",
       " 'bash',\n",
       " 'information_technology',\n",
       " 'myself',\n",
       " 'iodine',\n",
       " 'meter',\n",
       " 'angstrom',\n",
       " 'doctor',\n",
       " 'merely',\n",
       " 'iodine',\n",
       " 'just',\n",
       " 'can',\n",
       " 'thymine',\n",
       " 'range',\n",
       " 'information_technology',\n",
       " 'you',\n",
       " 'privation',\n",
       " 'Maine',\n",
       " 'to',\n",
       " 'sew',\n",
       " 'that',\n",
       " 'like',\n",
       " 'the',\n",
       " 'drape',\n",
       " 'iodine',\n",
       " 'use',\n",
       " 'angstrom',\n",
       " 'sew',\n",
       " 'machine',\n",
       " 'no',\n",
       " 'you',\n",
       " 'can',\n",
       " 'bash',\n",
       " 'this',\n",
       " 'iodine',\n",
       " 'meter',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'if',\n",
       " 'you',\n",
       " 'wouldn',\n",
       " 'thymine',\n",
       " 'mind',\n",
       " 'of',\n",
       " 'course',\n",
       " 'iodine',\n",
       " 'volition',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'information_technology',\n",
       " 'second',\n",
       " 'for',\n",
       " 'your',\n",
       " 'hand',\n",
       " 'save',\n",
       " 'Maine',\n",
       " 'some',\n",
       " 'for',\n",
       " 'the',\n",
       " 'wind',\n",
       " 'any',\n",
       " 'color',\n",
       " 'preference',\n",
       " 'no',\n",
       " 'standard',\n",
       " 'black',\n",
       " 'semen',\n",
       " 'on',\n",
       " 'hey',\n",
       " 'you',\n",
       " 'what',\n",
       " 'second',\n",
       " 'your',\n",
       " 'name',\n",
       " 'Maine',\n",
       " 'charlie',\n",
       " 'charlie',\n",
       " 'we',\n",
       " 'need',\n",
       " 'aid',\n",
       " 'with',\n",
       " 'the',\n",
       " 'fire',\n",
       " 'no',\n",
       " 'one',\n",
       " 'volition',\n",
       " 'see',\n",
       " 'information_technology',\n",
       " 'if',\n",
       " 'information_technology',\n",
       " 'International_Relations_and_Security_Network',\n",
       " 'thymine',\n",
       " 'large',\n",
       " 'O.K.',\n",
       " 'iodine',\n",
       " 'meter',\n",
       " 'on',\n",
       " 'information_technology',\n",
       " 'what',\n",
       " 'second',\n",
       " 'your',\n",
       " 'name',\n",
       " 'sayid',\n",
       " 'sayid',\n",
       " 'iodine',\n",
       " 'meter',\n",
       " 'on',\n",
       " 'information_technology',\n",
       " 'sayid',\n",
       " 'iodine',\n",
       " 'might',\n",
       " 'throw',\n",
       " 'up',\n",
       " 'on',\n",
       " 'you',\n",
       " 'you',\n",
       " 'rhenium',\n",
       " 'do',\n",
       " 'fine',\n",
       " 'you',\n",
       " 'Don',\n",
       " 'thymine',\n",
       " 'look',\n",
       " 'afraid',\n",
       " 'astatine',\n",
       " 'all',\n",
       " 'iodine',\n",
       " 'Don',\n",
       " 'thymine',\n",
       " 'understand',\n",
       " 'that',\n",
       " 'well',\n",
       " 'fear',\n",
       " 'second',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'Associate_in_Nursing',\n",
       " 'odd',\n",
       " 'thing',\n",
       " 'when',\n",
       " 'iodine',\n",
       " 'be',\n",
       " 'inch',\n",
       " 'residency',\n",
       " 'my',\n",
       " 'first',\n",
       " 'solo',\n",
       " 'procedurewas',\n",
       " 'angstrom',\n",
       " 'spinal_anesthesia',\n",
       " 'surgeryon',\n",
       " 'angstrom',\n",
       " 'year',\n",
       " 'old',\n",
       " 'child',\n",
       " 'angstrom',\n",
       " 'girl',\n",
       " 'and',\n",
       " 'astatine',\n",
       " 'the',\n",
       " 'end',\n",
       " 'after',\n",
       " 'hour',\n",
       " 'iodine',\n",
       " 'be',\n",
       " 'close',\n",
       " 'her',\n",
       " 'up',\n",
       " 'and',\n",
       " 'iodine',\n",
       " 'iodine',\n",
       " 'by_chance',\n",
       " 'rip',\n",
       " 'her',\n",
       " 'dural',\n",
       " 'sack',\n",
       " 'information_technology',\n",
       " 'second',\n",
       " 'astatine',\n",
       " 'the',\n",
       " 'base',\n",
       " 'of',\n",
       " 'the',\n",
       " 'spinewhere',\n",
       " 'all',\n",
       " 'the',\n",
       " 'nerve',\n",
       " 'semen',\n",
       " 'together',\n",
       " 'membrane',\n",
       " 'a',\n",
       " 'thin',\n",
       " 'a',\n",
       " 'tissue',\n",
       " 'and',\n",
       " 'sol',\n",
       " 'information_technology',\n",
       " 'rip',\n",
       " 'open',\n",
       " 'nerve',\n",
       " 'just',\n",
       " 'spilledout',\n",
       " 'of',\n",
       " 'her',\n",
       " 'like',\n",
       " 'angel',\n",
       " 'hair',\n",
       " 'pasta',\n",
       " 'spinal_anesthesia',\n",
       " 'fluid',\n",
       " 'flowingout',\n",
       " 'of',\n",
       " 'her',\n",
       " 'and',\n",
       " 'iodine',\n",
       " 'the',\n",
       " 'panic',\n",
       " 'be',\n",
       " 'just',\n",
       " 'sol',\n",
       " 'crazy',\n",
       " 'sol',\n",
       " 'real_number',\n",
       " 'and',\n",
       " 'iodine',\n",
       " 'know',\n",
       " 'iodine',\n",
       " 'have',\n",
       " 'to',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'information_technology',\n",
       " 'sol',\n",
       " 'iodine',\n",
       " 'just',\n",
       " 'make',\n",
       " 'angstrom',\n",
       " 'choice',\n",
       " 'iodine',\n",
       " 'vitamin_D',\n",
       " 'Lashkar-e-Taiba',\n",
       " 'the',\n",
       " 'fear',\n",
       " 'inch',\n",
       " 'Lashkar-e-Taiba',\n",
       " 'information_technology',\n",
       " 'return',\n",
       " 'over',\n",
       " 'Lashkar-e-Taiba',\n",
       " 'information_technology',\n",
       " 'bash',\n",
       " 'it',\n",
       " 'thing',\n",
       " 'merely',\n",
       " 'lone',\n",
       " 'for',\n",
       " 'five',\n",
       " 'second',\n",
       " 'that',\n",
       " 'second',\n",
       " 'all',\n",
       " 'iodine',\n",
       " 'be',\n",
       " 'gon',\n",
       " 'sodium',\n",
       " 'give',\n",
       " 'information_technology',\n",
       " 'sol',\n",
       " 'iodine',\n",
       " 'start',\n",
       " 'to',\n",
       " 'count',\n",
       " 'one',\n",
       " 'two',\n",
       " 'three',\n",
       " 'four',\n",
       " 'five',\n",
       " 'and',\n",
       " 'information_technology',\n",
       " 'be',\n",
       " 'go',\n",
       " 'iodine',\n",
       " 'go',\n",
       " 'back',\n",
       " 'to',\n",
       " 'work',\n",
       " 'sew',\n",
       " 'her',\n",
       " 'up',\n",
       " 'and',\n",
       " 'she',\n",
       " 'be',\n",
       " 'fine',\n",
       " 'if',\n",
       " 'that',\n",
       " 'have',\n",
       " 'be',\n",
       " 'Maine',\n",
       " 'iodine',\n",
       " 'thinki',\n",
       " 'would',\n",
       " 've',\n",
       " 'run',\n",
       " 'for',\n",
       " 'the',\n",
       " 'door',\n",
       " 'no',\n",
       " 'iodine',\n",
       " 'Don',\n",
       " 'thymine',\n",
       " 'think',\n",
       " 'that',\n",
       " 'second',\n",
       " 'true',\n",
       " 'you',\n",
       " 'rhenium',\n",
       " 'not',\n",
       " 'run',\n",
       " 'now',\n",
       " 'you',\n",
       " 'think',\n",
       " 'theywould',\n",
       " 'rich_person',\n",
       " 'semen',\n",
       " 'by',\n",
       " 'now',\n",
       " 'hmm',\n",
       " 'World_Health_Organization',\n",
       " 'anyone',\n",
       " 'a',\n",
       " 'if',\n",
       " 'iodine',\n",
       " 'meter',\n",
       " 'gon',\n",
       " 'sodium',\n",
       " 'start',\n",
       " 'eat',\n",
       " 'cocoa',\n",
       " 'Shannon',\n",
       " 'we',\n",
       " 'May',\n",
       " 'beryllium',\n",
       " 'here',\n",
       " 'for',\n",
       " 'angstrom',\n",
       " 'while',\n",
       " 'the',\n",
       " 'airplane',\n",
       " 'have',\n",
       " 'angstrom',\n",
       " 'black',\n",
       " 'box',\n",
       " 'idiot',\n",
       " 'they',\n",
       " 'know',\n",
       " 'precisely',\n",
       " 'where',\n",
       " 'we',\n",
       " 'be',\n",
       " 'they',\n",
       " 'rhenium',\n",
       " 'come',\n",
       " 'iodine',\n",
       " 'll',\n",
       " 'eat',\n",
       " 'on',\n",
       " 'the',\n",
       " 'rescue',\n",
       " 'boat',\n",
       " 'iodine',\n",
       " 'll',\n",
       " 'eat',\n",
       " 'on',\n",
       " 'the',\n",
       " 'rescue',\n",
       " 'boat',\n",
       " 'hungry',\n",
       " 'yea',\n",
       " 'thank',\n",
       " 'any',\n",
       " 'More',\n",
       " 'you',\n",
       " 'know',\n",
       " 'baby',\n",
       " 'material',\n",
       " 'no',\n",
       " 'iodine',\n",
       " 'meter',\n",
       " 'O.K.',\n",
       " 'well',\n",
       " 'bent',\n",
       " 'inch',\n",
       " 'there',\n",
       " 'yea',\n",
       " 'you',\n",
       " 'excessively',\n",
       " 'you',\n",
       " 'certain',\n",
       " 'you',\n",
       " 'rhenium',\n",
       " 'warm',\n",
       " 'enough',\n",
       " 'bash',\n",
       " 'you',\n",
       " 'think',\n",
       " 'helium',\n",
       " 'second',\n",
       " 'gon',\n",
       " 'sodium',\n",
       " 'populate',\n",
       " 'bash',\n",
       " 'you',\n",
       " 'know',\n",
       " 'him',\n",
       " 'helium',\n",
       " 'be',\n",
       " 'sit',\n",
       " 'following',\n",
       " 'to',\n",
       " 'Maine',\n",
       " 'we',\n",
       " 'must',\n",
       " 've',\n",
       " 'be',\n",
       " 'astatine',\n",
       " 'about',\n",
       " 'foot',\n",
       " 'when',\n",
       " 'information_technology',\n",
       " 'happen',\n",
       " 'we',\n",
       " 'hit',\n",
       " 'Associate_in_Nursing',\n",
       " 'air',\n",
       " 'pocket',\n",
       " 'and',\n",
       " 'drop',\n",
       " 'possibly',\n",
       " 'foot',\n",
       " 'the',\n",
       " 'turbulence',\n",
       " 'be',\n",
       " 'iodine',\n",
       " 'black',\n",
       " 'out',\n",
       " 'iodine',\n",
       " 'didn',\n",
       " 'thymine',\n",
       " 'iodine',\n",
       " 'proverb',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'thing',\n",
       " 'iodine',\n",
       " 'know',\n",
       " 'that',\n",
       " 'the',\n",
       " 'tail',\n",
       " 'be',\n",
       " 'go',\n",
       " 'merely',\n",
       " 'iodine',\n",
       " 'couldn',\n",
       " 'thymine',\n",
       " 'bring',\n",
       " 'myself',\n",
       " 'to',\n",
       " 'expression',\n",
       " 'back',\n",
       " 'and',\n",
       " 'then',\n",
       " 'the',\n",
       " 'front',\n",
       " 'endof',\n",
       " 'the',\n",
       " 'airplane',\n",
       " 'break',\n",
       " 'murder',\n",
       " 'well',\n",
       " 'information_technology',\n",
       " 'second',\n",
       " 'not',\n",
       " 'here',\n",
       " 'on',\n",
       " 'the',\n",
       " 'beach',\n",
       " 'neither',\n",
       " 'be',\n",
       " 'the',\n",
       " 'tail',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'figure',\n",
       " 'outwhich',\n",
       " 'manner',\n",
       " 'we',\n",
       " 'come',\n",
       " 'inch',\n",
       " 'why',\n",
       " 'there',\n",
       " 'second',\n",
       " 'angstrom',\n",
       " 'opportunity',\n",
       " 'we',\n",
       " 'discovery',\n",
       " 'the',\n",
       " 'cockpit',\n",
       " 'if',\n",
       " 'information_technology',\n",
       " 'second',\n",
       " 'integral',\n",
       " 'we',\n",
       " 'might',\n",
       " 'beryllium',\n",
       " 'ableto',\n",
       " 'discovery',\n",
       " 'the',\n",
       " 'transceiver',\n",
       " 'we',\n",
       " 'could',\n",
       " 'send',\n",
       " 'out',\n",
       " 'angstrom',\n",
       " 'signal',\n",
       " 'aid',\n",
       " 'the',\n",
       " 'rescue',\n",
       " 'party',\n",
       " 'discovery',\n",
       " 'u',\n",
       " 'how',\n",
       " 'bash',\n",
       " 'you',\n",
       " 'know',\n",
       " 'all',\n",
       " 'that',\n",
       " 'iodine',\n",
       " 'take',\n",
       " 'angstrom',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'fly',\n",
       " 'lesson',\n",
       " 'wasn',\n",
       " 'thymine',\n",
       " 'for',\n",
       " 'Maine',\n",
       " 'iodine',\n",
       " 'proverb',\n",
       " 'some',\n",
       " 'smoke',\n",
       " 'just',\n",
       " 'done',\n",
       " 'the',\n",
       " 'valley',\n",
       " 'if',\n",
       " 'you',\n",
       " 'rhenium',\n",
       " 'think',\n",
       " 'about',\n",
       " 'goingfor',\n",
       " 'the',\n",
       " 'cockpit',\n",
       " 'iodine',\n",
       " 'meter',\n",
       " 'go',\n",
       " 'with',\n",
       " 'you',\n",
       " 'iodine',\n",
       " 'Don',\n",
       " 'thymine',\n",
       " 'know',\n",
       " 'your',\n",
       " 'name',\n",
       " 'iodine',\n",
       " 'meter',\n",
       " 'kate',\n",
       " 'jack',\n",
       " 'what',\n",
       " 'be',\n",
       " 'that',\n",
       " 'that',\n",
       " 'be',\n",
       " 'Wyrd',\n",
       " 'right',\n",
       " 'be',\n",
       " 'that',\n",
       " 'vincent',\n",
       " 'information_technology',\n",
       " 'second',\n",
       " 'not',\n",
       " 'vincent',\n",
       " 'do',\n",
       " 'anybody',\n",
       " 'see',\n",
       " 'that',\n",
       " 'yea',\n",
       " 'Boone',\n",
       " 'terrific',\n",
       " 'sol',\n",
       " 'how',\n",
       " 'second',\n",
       " 'the',\n",
       " 'drink',\n",
       " 'information_technology',\n",
       " 'second',\n",
       " 'good',\n",
       " 'that',\n",
       " 'wasn',\n",
       " 'thymine',\n",
       " 'angstrom',\n",
       " 'very',\n",
       " 'strong',\n",
       " 'chemical_reaction',\n",
       " 'well',\n",
       " 'information_technology',\n",
       " 'second',\n",
       " 'not',\n",
       " 'angstrom',\n",
       " 'very',\n",
       " 'strong',\n",
       " 'drink',\n",
       " 'just',\n",
       " 'Don',\n",
       " 'thymine',\n",
       " 'Tell',\n",
       " 'anyone',\n",
       " 'this',\n",
       " 'of',\n",
       " 'course',\n",
       " 'break',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTranscriptsLemanized = nlt.lemanization(dfTranscriptsTokenized)\n",
    "#dfTranscriptsLemanized = dfTranscriptsTokenized.drop('transcripts_prepared_for_tokenization', axis=1)\n",
    "dfTranscriptsLemanized.iloc[0][\"transcript_lemanized\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iodine': 173,\n",
       " 'previously': 1,\n",
       " 'on': 18,\n",
       " 'lose': 3,\n",
       " 'bash': 21,\n",
       " 'you': 118,\n",
       " 'think': 6,\n",
       " 'helium': 42,\n",
       " 'second': 54,\n",
       " 'gon': 23,\n",
       " 'sodium': 24,\n",
       " 'populate': 1,\n",
       " 'know': 22,\n",
       " 'him': 10,\n",
       " 'be': 54,\n",
       " 'sit': 2,\n",
       " 'following': 1,\n",
       " 'to': 62,\n",
       " 'Maine': 25,\n",
       " 'theywould': 1,\n",
       " 'rich_person': 14,\n",
       " 'semen': 3,\n",
       " 'by': 2,\n",
       " 'now': 5,\n",
       " 'World_Health_Organization': 10,\n",
       " 'anyone': 1,\n",
       " 'what': 52,\n",
       " 'information_technology': 57,\n",
       " 'like': 5,\n",
       " 'checker': 2,\n",
       " 'not': 23,\n",
       " 'truly': 5,\n",
       " 'angstrom': 57,\n",
       " 'good': 10,\n",
       " 'game': 1,\n",
       " 'than': 3,\n",
       " 'two': 4,\n",
       " 'player': 1,\n",
       " 'side': 4,\n",
       " 'one': 17,\n",
       " 'light': 2,\n",
       " 'dark': 3,\n",
       " 'walt': 4,\n",
       " 'wide_area_network': 1,\n",
       " 'secret': 3,\n",
       " 'Don': 20,\n",
       " 'thymine': 49,\n",
       " 'trust': 4,\n",
       " 'her': 11,\n",
       " 'she': 21,\n",
       " 'dangerous': 4,\n",
       " 'attempt': 3,\n",
       " 'move': 1,\n",
       " 'man': 4,\n",
       " 'discovery': 5,\n",
       " 'bring': 2,\n",
       " 'back': 11,\n",
       " 'yea': 28,\n",
       " 'support': 5,\n",
       " 'say': 16,\n",
       " 'that': 30,\n",
       " 'every': 2,\n",
       " 'time': 5,\n",
       " 'ask': 6,\n",
       " 'tobring': 1,\n",
       " 'pas': 1,\n",
       " 'out': 13,\n",
       " 'again': 2,\n",
       " 'since': 1,\n",
       " 'your': 24,\n",
       " 'fever': 3,\n",
       " 'pretty': 2,\n",
       " 'high': 1,\n",
       " 'meter': 21,\n",
       " 'guess': 3,\n",
       " 'no': 17,\n",
       " 'idea': 1,\n",
       " 'the': 79,\n",
       " 'hell': 6,\n",
       " 'rhenium': 28,\n",
       " 'talk': 5,\n",
       " 'about': 13,\n",
       " 'my': 18,\n",
       " 'cuff': 2,\n",
       " 'handcuff': 2,\n",
       " 'where': 7,\n",
       " 'jacket': 2,\n",
       " 'pocket': 2,\n",
       " 'O.K.': 9,\n",
       " 'get': 41,\n",
       " 'choice': 2,\n",
       " 'up': 10,\n",
       " 'pace': 1,\n",
       " 'yokel': 1,\n",
       " 'little': 2,\n",
       " 'loud': 1,\n",
       " 'we': 42,\n",
       " 'should': 9,\n",
       " 'brand': 3,\n",
       " 'camp': 1,\n",
       " 'here': 9,\n",
       " 'yes': 1,\n",
       " 'stop': 3,\n",
       " 'Nice': 3,\n",
       " 'cookout': 1,\n",
       " 'walk': 6,\n",
       " 'done': 2,\n",
       " 'jungle': 2,\n",
       " 'inch': 26,\n",
       " 'afraid': 1,\n",
       " 'tree': 3,\n",
       " 'volition': 11,\n",
       " 'u': 6,\n",
       " 'knockingdown': 1,\n",
       " 'sol': 21,\n",
       " 'worry': 2,\n",
       " 'how': 13,\n",
       " 'aboutyou': 1,\n",
       " 'give': 6,\n",
       " 'cartridge_holder': 1,\n",
       " 'put_option': 2,\n",
       " 'gun': 9,\n",
       " 'pant': 1,\n",
       " 'sayid': 1,\n",
       " 'right': 6,\n",
       " 'beach': 1,\n",
       " 'why': 6,\n",
       " 'this': 8,\n",
       " 'Australia': 2,\n",
       " 'stick': 1,\n",
       " 'day': 4,\n",
       " 'ago': 3,\n",
       " 'return': 7,\n",
       " 'murder': 6,\n",
       " 'from': 8,\n",
       " 'Sydney': 1,\n",
       " 'fly': 1,\n",
       " 'along': 2,\n",
       " 'Lapp': 3,\n",
       " 'northeast': 1,\n",
       " 'routeevery': 1,\n",
       " 'commercial': 1,\n",
       " 'airlinerbound': 1,\n",
       " 'for': 20,\n",
       " 'los': 1,\n",
       " 'angeles': 1,\n",
       " 'do': 20,\n",
       " 'pilot': 2,\n",
       " 'lostcommunication': 1,\n",
       " 'with': 9,\n",
       " 'grind': 1,\n",
       " 'six': 1,\n",
       " 'hour': 2,\n",
       " 'turn': 1,\n",
       " 'and': 25,\n",
       " 'head': 2,\n",
       " 'Fiji': 1,\n",
       " 'change': 2,\n",
       " 'course': 3,\n",
       " 'unfortunately': 1,\n",
       " 'turbulence': 1,\n",
       " 'hit': 1,\n",
       " 'remainder': 1,\n",
       " 'wereover': 1,\n",
       " 'mile': 1,\n",
       " 'they': 19,\n",
       " 'll': 11,\n",
       " 'satellite': 2,\n",
       " 'cantake': 1,\n",
       " 'picture': 3,\n",
       " 'of': 24,\n",
       " 'license': 2,\n",
       " 'home_plate': 1,\n",
       " 'if': 10,\n",
       " 'lone': 4,\n",
       " 'allwearing': 1,\n",
       " 'plat': 1,\n",
       " 'aren': 1,\n",
       " 'pessimist': 1,\n",
       " 'BASIC': 1,\n",
       " 'photography': 1,\n",
       " 'point': 2,\n",
       " 'shoot': 5,\n",
       " 'can': 9,\n",
       " 'merely': 16,\n",
       " 'must': 1,\n",
       " 'beryllium': 10,\n",
       " 'tell': 4,\n",
       " 'Ohio': 13,\n",
       " 'bollock': 1,\n",
       " 'enjoyedthe': 1,\n",
       " 'puppet': 1,\n",
       " 'show': 1,\n",
       " 'antic': 1,\n",
       " 'stuckin': 1,\n",
       " 'center': 2,\n",
       " 'damn': 7,\n",
       " 'nowhere': 1,\n",
       " 'talkabout': 1,\n",
       " 'other': 2,\n",
       " 'thing': 5,\n",
       " 'transmission': 2,\n",
       " 'abdulpicked': 1,\n",
       " 'his': 16,\n",
       " 'radio': 1,\n",
       " 'French': 2,\n",
       " 'chick': 1,\n",
       " 'all': 9,\n",
       " 'dead': 4,\n",
       " 'cringle': 1,\n",
       " 'hanker': 2,\n",
       " 'freckle': 1,\n",
       " 'sixteen': 1,\n",
       " 'year': 2,\n",
       " 'Lashkar-e-Taiba': 5,\n",
       " 'Tell': 12,\n",
       " 'otherswhen': 1,\n",
       " 'them': 5,\n",
       " 'precisely': 1,\n",
       " 'hear': 6,\n",
       " 'stupid': 1,\n",
       " 'translator': 1,\n",
       " 'anything': 9,\n",
       " 'relay': 1,\n",
       " 'without': 1,\n",
       " 'fullyunderstanding': 1,\n",
       " 'cause': 6,\n",
       " 'panic': 1,\n",
       " 'away': 16,\n",
       " 'their': 1,\n",
       " 'hope': 2,\n",
       " 'verydangerous': 1,\n",
       " 'lie': 1,\n",
       " 'dinosaur': 4,\n",
       " 'wasn': 2,\n",
       " 'didn': 7,\n",
       " 'see': 5,\n",
       " 'knowit': 1,\n",
       " 'because': 1,\n",
       " 'extinct': 1,\n",
       " 'narrative': 1,\n",
       " 'look': 6,\n",
       " 'kind': 2,\n",
       " 'die': 5,\n",
       " 'yellow': 1,\n",
       " 'wind': 1,\n",
       " 'infect': 1,\n",
       " 'antibiotic': 3,\n",
       " 'battle': 1,\n",
       " 'body': 2,\n",
       " 'close': 1,\n",
       " 'down': 2,\n",
       " 'piece': 2,\n",
       " 'astatine': 2,\n",
       " 'abdomen': 2,\n",
       " 'go': 10,\n",
       " 'rigid': 2,\n",
       " 'then': 2,\n",
       " 'pain': 1,\n",
       " 'uh': 13,\n",
       " 'fellow': 4,\n",
       " 'none': 1,\n",
       " 'business': 3,\n",
       " 'hard-core': 1,\n",
       " 'hurley': 2,\n",
       " 'stand': 1,\n",
       " 'guard': 1,\n",
       " 'there': 10,\n",
       " 'take': 3,\n",
       " 'male_child': 3,\n",
       " 'doesn': 3,\n",
       " 'believe': 1,\n",
       " 'march': 1,\n",
       " 'aljazeera': 2,\n",
       " 'protect': 1,\n",
       " 'network': 1,\n",
       " 'kate': 9,\n",
       " 'clasp': 1,\n",
       " 'fine': 1,\n",
       " 'well': 7,\n",
       " 'morning': 3,\n",
       " 'sleep': 1,\n",
       " 'sheep': 1,\n",
       " 'pen': 1,\n",
       " 'regretful': 3,\n",
       " 'vitamin_D': 7,\n",
       " 'town': 2,\n",
       " 'near': 1,\n",
       " 'kilometer': 1,\n",
       " 'possibly': 4,\n",
       " 'exhaust': 1,\n",
       " 'name': 2,\n",
       " 'annie': 5,\n",
       " 'hungry': 2,\n",
       " 'privation': 7,\n",
       " 'whyyou': 1,\n",
       " 'trespass': 1,\n",
       " 'property': 1,\n",
       " 'run': 2,\n",
       " 'money': 1,\n",
       " 'Associate_in_Nursing': 3,\n",
       " 'American': 1,\n",
       " 'Canadian': 1,\n",
       " 'graduate': 1,\n",
       " 'collegeand': 1,\n",
       " 'figure': 3,\n",
       " 'universe': 1,\n",
       " 'top': 1,\n",
       " 'list': 1,\n",
       " 'hop': 2,\n",
       " 'flight': 2,\n",
       " 'Melbourne': 2,\n",
       " 'anybody': 1,\n",
       " 'while': 1,\n",
       " 'hundredkilometers': 1,\n",
       " 'just': 11,\n",
       " 'happenedto': 1,\n",
       " 'roll': 1,\n",
       " 'onto': 2,\n",
       " 'farm': 2,\n",
       " 'excessively': 5,\n",
       " 'work': 1,\n",
       " 'wife': 1,\n",
       " 'diedeight': 1,\n",
       " 'month': 2,\n",
       " 'Wednesday': 1,\n",
       " 'leave': 3,\n",
       " 'many': 4,\n",
       " 'choresand': 1,\n",
       " 'mortgage': 3,\n",
       " 'aid': 3,\n",
       " 'first': 1,\n",
       " 'carnival': 1,\n",
       " 'wageand': 1,\n",
       " 'topographic_point': 2,\n",
       " 'stay': 3,\n",
       " 'deal': 1,\n",
       " 'left-hander': 1,\n",
       " 'hey': 7,\n",
       " 'a': 7,\n",
       " 'others': 3,\n",
       " 'hike': 2,\n",
       " 'mountainin': 1,\n",
       " 'helpthe': 1,\n",
       " 'rescue': 2,\n",
       " 'team': 1,\n",
       " 'locate': 1,\n",
       " 'transceiver': 1,\n",
       " 'failedto': 1,\n",
       " 'signal': 4,\n",
       " 'weren': 2,\n",
       " 'able': 2,\n",
       " 'send': 2,\n",
       " 'outa': 1,\n",
       " 'call': 1,\n",
       " 'gather': 1,\n",
       " 'electronic': 1,\n",
       " 'equipment': 1,\n",
       " 'cell': 1,\n",
       " 'phone': 1,\n",
       " 'laptop': 1,\n",
       " 'boost': 1,\n",
       " 'May': 1,\n",
       " 'some': 5,\n",
       " 'beginrationing': 1,\n",
       " 'our': 1,\n",
       " 'remain': 1,\n",
       " 'food': 2,\n",
       " 'rain': 3,\n",
       " 'set': 2,\n",
       " 'uptarps': 1,\n",
       " 'collect': 1,\n",
       " 'water': 6,\n",
       " 'need': 9,\n",
       " 'organizethree': 1,\n",
       " 'offprint': 1,\n",
       " 'group': 4,\n",
       " 'each': 1,\n",
       " 'leader': 1,\n",
       " 'form': 1,\n",
       " 'gonnaorganize': 1,\n",
       " 'electronics': 1,\n",
       " 'ration': 1,\n",
       " 'one-third': 1,\n",
       " 'concernthemselves': 1,\n",
       " 'construction': 1,\n",
       " 'hello': 3,\n",
       " 'liter': 4,\n",
       " 'something': 5,\n",
       " 'certain': 3,\n",
       " 'sun': 1,\n",
       " 'couldn': 2,\n",
       " 'signalbecause': 1,\n",
       " 'wasanother': 1,\n",
       " 'block': 1,\n",
       " 'distress': 1,\n",
       " 'callfrom': 1,\n",
       " 'woman': 1,\n",
       " 'have': 7,\n",
       " 'kill': 1,\n",
       " 'alone': 3,\n",
       " 'island': 1,\n",
       " 'play': 1,\n",
       " 'jack': 2,\n",
       " 'want': 4,\n",
       " 'else': 1,\n",
       " 'shrapnel': 1,\n",
       " 'touch': 1,\n",
       " 'aftermath': 2,\n",
       " 'few': 1,\n",
       " 'secondsduring': 1,\n",
       " 'surgery': 1,\n",
       " 'rather': 1,\n",
       " 'knowshe': 1,\n",
       " 'mug': 2,\n",
       " 'find': 2,\n",
       " 'those': 2,\n",
       " 'guy': 7,\n",
       " 'keep': 2,\n",
       " 'mumble': 1,\n",
       " 'over': 5,\n",
       " 'problem': 1,\n",
       " 'Rebel': 1,\n",
       " 'careof': 1,\n",
       " 'when': 2,\n",
       " 'betterif': 1,\n",
       " 'strong': 1,\n",
       " 'material': 1,\n",
       " 'baggage': 1,\n",
       " 'ear': 1,\n",
       " 'infection': 1,\n",
       " 'foot': 1,\n",
       " 'fungus': 1,\n",
       " 'everything': 1,\n",
       " 'luggagein': 1,\n",
       " 'operating_expense': 1,\n",
       " 'compartment': 1,\n",
       " 'inside': 2,\n",
       " 'airplane': 2,\n",
       " 'handle': 1,\n",
       " 'eye': 2,\n",
       " 'great': 1,\n",
       " 'love': 1,\n",
       " 'boo': 1,\n",
       " 'trick': 1,\n",
       " 'Oregon': 2,\n",
       " 'dainty': 1,\n",
       " 'loot': 1,\n",
       " 'ah': 1,\n",
       " 'potato': 1,\n",
       " 'bag': 1,\n",
       " 'liquor': 1,\n",
       " 'smoke': 1,\n",
       " 'couple': 2,\n",
       " 'playboy': 1,\n",
       " 'yours': 1,\n",
       " 'medicine': 2,\n",
       " 'sum': 1,\n",
       " 'home': 1,\n",
       " 'bargain': 1,\n",
       " 'brother': 1,\n",
       " 'upand': 1,\n",
       " 'smell': 1,\n",
       " 'chump': 3,\n",
       " 'crap': 1,\n",
       " 'own': 3,\n",
       " 'come': 3,\n",
       " 'waste': 1,\n",
       " 'try': 2,\n",
       " 'save': 4,\n",
       " 'stopping_point': 2,\n",
       " 'check': 2,\n",
       " 'metallic_element': 1,\n",
       " 'size': 1,\n",
       " 'headsticking': 1,\n",
       " 'breadbasket': 1,\n",
       " 'pillsyou': 1,\n",
       " 'use': 2,\n",
       " 'fix': 1,\n",
       " 'lookingat': 1,\n",
       " 'large': 1,\n",
       " 'doctor': 2,\n",
       " 'still': 2,\n",
       " 'civilization': 1,\n",
       " 'wild': 1,\n",
       " 'whoa': 2,\n",
       " 'thank': 4,\n",
       " 'expression': 6,\n",
       " 'bright': 1,\n",
       " 'whoever': 1,\n",
       " 'isprobably': 1,\n",
       " 'baby': 1,\n",
       " 'husband': 1,\n",
       " 'marry': 1,\n",
       " 'modern': 1,\n",
       " 'work_force': 1,\n",
       " 'bloody': 1,\n",
       " 'useless': 1,\n",
       " 'manner': 2,\n",
       " 'upthat': 1,\n",
       " 'mountain': 1,\n",
       " 'nothing': 1,\n",
       " 'huh': 2,\n",
       " 'More': 5,\n",
       " 'haven': 2,\n",
       " 'meet': 2,\n",
       " 'might': 1,\n",
       " 'fuselage': 1,\n",
       " 'bank': 1,\n",
       " 'would': 4,\n",
       " 'havegiven': 1,\n",
       " 'heck': 1,\n",
       " 'toasterif': 1,\n",
       " 'thatin': 1,\n",
       " 'history': 1,\n",
       " 'toaster': 1,\n",
       " 'hide': 1,\n",
       " 'wagesin': 1,\n",
       " 'tin': 1,\n",
       " 'mean': 3,\n",
       " 'haveheld': 1,\n",
       " 'issue': 1,\n",
       " 'adieu': 1,\n",
       " 'write': 1,\n",
       " 'note': 1,\n",
       " 've': 3,\n",
       " 'herealmost': 1,\n",
       " 'three': 3,\n",
       " 'timei': 1,\n",
       " 'yourself': 1,\n",
       " 'mind': 2,\n",
       " 'onthe': 2,\n",
       " 'wrong': 1,\n",
       " 'bad': 1,\n",
       " 'relationship': 1,\n",
       " 'always': 1,\n",
       " 'knewyou': 2,\n",
       " 'someday': 1,\n",
       " 'wouldn': 2,\n",
       " 'bein': 1,\n",
       " 'night': 2,\n",
       " 'youjust': 1,\n",
       " 'drive': 1,\n",
       " 'train': 2,\n",
       " 'stationfirst': 1,\n",
       " 'promise': 1,\n",
       " 'everyone': 2,\n",
       " 'deserve': 1,\n",
       " 'fresh': 1,\n",
       " 'start': 2,\n",
       " 'breathe': 2,\n",
       " 'jump': 1,\n",
       " 'grab': 1,\n",
       " 'respond': 1,\n",
       " 'bleed': 1,\n",
       " 'internally': 1,\n",
       " 'push': 1,\n",
       " 'suffer': 1,\n",
       " 'quick': 2,\n",
       " 'win': 2,\n",
       " 'four': 1,\n",
       " 'feel': 4,\n",
       " 'misery': 1,\n",
       " 'proverb': 3,\n",
       " 'murderer': 1,\n",
       " 'leavin': 1,\n",
       " 'listen': 5,\n",
       " 'clinein': 1,\n",
       " 'Canada': 1,\n",
       " 'Cline': 1,\n",
       " 'everywhere': 1,\n",
       " 'eat': 1,\n",
       " 'heremakes': 1,\n",
       " 'Burger': 1,\n",
       " 'beam': 2,\n",
       " 'post': 1,\n",
       " 'office': 1,\n",
       " 'nether': 1,\n",
       " 'wages': 1,\n",
       " 'make': 3,\n",
       " 'any': 2,\n",
       " 'difficult': 1,\n",
       " 'decision': 1,\n",
       " 'hang': 2,\n",
       " 'bald': 1,\n",
       " 'Mister': 4,\n",
       " 'Locke': 4,\n",
       " 'kid': 1,\n",
       " 'Sayda': 1,\n",
       " 'miracle': 1,\n",
       " 'happen': 1,\n",
       " 'miraclehappened': 1,\n",
       " 'survive': 1,\n",
       " 'clang': 3,\n",
       " 'friend': 4,\n",
       " 'vincent': 4,\n",
       " 'dog': 6,\n",
       " 'everythingi': 1,\n",
       " 'care': 1,\n",
       " 'backas': 1,\n",
       " 'soon': 2,\n",
       " 'walkingthrough': 1,\n",
       " 'haunt': 1,\n",
       " 'buddy': 1,\n",
       " 'chase': 1,\n",
       " 'yttrium': 1,\n",
       " 'probably': 1,\n",
       " 'whistle': 1,\n",
       " 'tribal': 1,\n",
       " 'flute': 1,\n",
       " 'oncein': 1,\n",
       " 'record': 1,\n",
       " 'session': 1,\n",
       " 'wish': 1,\n",
       " 'already': 1,\n",
       " 'real_number': 1,\n",
       " 'humanist': 1,\n",
       " 'Shannon': 1,\n",
       " 'disturbance': 1,\n",
       " 'wantto': 1,\n",
       " 'tent': 2,\n",
       " 'life': 1,\n",
       " 'rumor': 1,\n",
       " 'stickin': 1,\n",
       " 'denim': 1,\n",
       " 'Bethe': 1,\n",
       " 'listeningto': 1,\n",
       " 'poor_people': 1,\n",
       " 'scream': 1,\n",
       " 'nightknows': 1,\n",
       " 'act': 1,\n",
       " 'surprise': 1,\n",
       " 'heardyou': 1,\n",
       " 'hero': 1,\n",
       " 'bullet': 2,\n",
       " 'approach': 1,\n",
       " 'poetic': 1,\n",
       " 'lull': 1,\n",
       " 'matter': 3,\n",
       " 'word': 1,\n",
       " 'favor': 3,\n",
       " 'knowwhat': 1,\n",
       " 'earlier': 3,\n",
       " 'captain': 1,\n",
       " 'switch': 1,\n",
       " 'fasten': 2,\n",
       " 'seat': 2,\n",
       " 'belt': 2,\n",
       " 'sign': 1,\n",
       " 'tax_return': 1,\n",
       " 'seatsand': 1,\n",
       " 'ought': 1,\n",
       " 'surethat': 1,\n",
       " 'mullen': 1,\n",
       " 'thousand': 1,\n",
       " 'rat': 1,\n",
       " 'awayif': 1,\n",
       " 'hadn': 2,\n",
       " 'case': 1,\n",
       " 'notice': 1,\n",
       " 'free': 1,\n",
       " 'yo': 1,\n",
       " 'fugitive': 1,\n",
       " 'pound': 1,\n",
       " 'soak': 1,\n",
       " 'moisture': 1,\n",
       " 'strap': 1,\n",
       " 'itany': 1,\n",
       " 'thorax': 1,\n",
       " 'aim': 1,\n",
       " 'heart': 1,\n",
       " 'miss': 1,\n",
       " 'perforate': 1,\n",
       " 'lung': 1,\n",
       " 'shed_blood': 1,\n",
       " 'shh': 1,\n",
       " 'son': 1,\n",
       " 'tether': 1,\n",
       " 'himto': 1,\n",
       " 'ma': 1,\n",
       " 'oneto': 1,\n",
       " 'welcome': 1,\n",
       " 'after': 1,\n",
       " 'joe': 1,\n",
       " 'purdy': 1,\n",
       " 'wash': 10,\n",
       " 'trouble': 2,\n",
       " 'Godhead': 5,\n",
       " 'today': 5,\n",
       " 'sin': 1,\n",
       " 'cryin': 4,\n",
       " 'old': 1,\n",
       " 'rivergonna': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlt.get_occurence_from_list_lem(dfTranscriptsLemanized.iloc[2]['transcript_lemanized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lost</th>\n",
       "      <th>Heroes</th>\n",
       "      <th>Jericho_(2006)</th>\n",
       "      <th>Prison_Break</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aid</th>\n",
       "      <td>745.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maine</th>\n",
       "      <td>4443.0</td>\n",
       "      <td>3374.0</td>\n",
       "      <td>877.0</td>\n",
       "      <td>2414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person</th>\n",
       "      <td>448.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>292.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out</th>\n",
       "      <td>1479.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>703.0</td>\n",
       "      <td>1410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>over</th>\n",
       "      <td>497.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>328.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>throughpush</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>importantwhat</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>safeguard</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>placefaster</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illinoiscould</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38186 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Lost  Heroes  Jericho_(2006)  Prison_Break\n",
       "aid             745.0   610.0           230.0         309.0\n",
       "Maine          4443.0  3374.0           877.0        2414.0\n",
       "person          448.0   348.0           138.0         292.0\n",
       "out            1479.0  1028.0           703.0        1410.0\n",
       "over            497.0   280.0           158.0         328.0\n",
       "...               ...     ...             ...           ...\n",
       "throughpush       NaN     NaN             NaN           1.0\n",
       "importantwhat     NaN     NaN             NaN           1.0\n",
       "safeguard         NaN     NaN             NaN           1.0\n",
       "placefaster       NaN     NaN             NaN           1.0\n",
       "illinoiscould     NaN     NaN             NaN           1.0\n",
       "\n",
       "[38186 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOccurenceSerie = nlt.get_occurence_from_df_serie(dfTranscriptsLemanized)\n",
    "dfOccurenceSerie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e87ed7f3962434517c25eb050070d235fd9b8942ce30e13884a6950b582033b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
