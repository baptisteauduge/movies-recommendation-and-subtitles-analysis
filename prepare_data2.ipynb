{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.import_data as import_data\n",
    "import modules.natural_language_treatment as nlt\n",
    "from swifter import set_defaults\n",
    "set_defaults(\n",
    "    npartitions=30,\n",
    "    dask_threshold=1,\n",
    "    scheduler=\"threads\",\n",
    "    progress_bar=True,\n",
    "    progress_bar_desc=False,\n",
    "    allow_dask_on_strings=True,\n",
    "    force_parallel=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère l'ensemble des séries contenus dans notre base de donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serie</th>\n",
       "      <th>season</th>\n",
       "      <th>episode_number</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot_(1)</td>\n",
       "      <td>data/transcripts/1___Lost/01/01__Pilot_(1).txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>2</td>\n",
       "      <td>Pilot_(2)</td>\n",
       "      <td>data/transcripts/1___Lost/01/02__Pilot_(2).txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>3</td>\n",
       "      <td>Tabula_rasa</td>\n",
       "      <td>data/transcripts/1___Lost/01/03__Tabula_rasa.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>4</td>\n",
       "      <td>Walkabout</td>\n",
       "      <td>data/transcripts/1___Lost/01/04__Walkabout.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>5</td>\n",
       "      <td>White_rabbit</td>\n",
       "      <td>data/transcripts/1___Lost/01/05__White_rabbit.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81817</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>3</td>\n",
       "      <td>Rome</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/03__...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81818</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>4</td>\n",
       "      <td>Hunger</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/04__...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81819</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>5</td>\n",
       "      <td>Past</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/05__...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81820</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>6</td>\n",
       "      <td>Hustle</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/06__...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81821</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>7</td>\n",
       "      <td>Napkin</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/07__...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81822 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                serie season  episode_number  episode_name  \\\n",
       "0                Lost     01               1     Pilot_(1)   \n",
       "1                Lost     01               2     Pilot_(2)   \n",
       "2                Lost     01               3   Tabula_rasa   \n",
       "3                Lost     01               4     Walkabout   \n",
       "4                Lost     01               5  White_rabbit   \n",
       "...               ...    ...             ...           ...   \n",
       "81817  Mind_of_a_Chef     04               3          Rome   \n",
       "81818  Mind_of_a_Chef     04               4        Hunger   \n",
       "81819  Mind_of_a_Chef     04               5          Past   \n",
       "81820  Mind_of_a_Chef     04               6        Hustle   \n",
       "81821  Mind_of_a_Chef     04               7        Napkin   \n",
       "\n",
       "                                                    path  \n",
       "0         data/transcripts/1___Lost/01/01__Pilot_(1).txt  \n",
       "1         data/transcripts/1___Lost/01/02__Pilot_(2).txt  \n",
       "2       data/transcripts/1___Lost/01/03__Tabula_rasa.txt  \n",
       "3         data/transcripts/1___Lost/01/04__Walkabout.txt  \n",
       "4      data/transcripts/1___Lost/01/05__White_rabbit.txt  \n",
       "...                                                  ...  \n",
       "81817  data/transcripts/5480___Mind_of_a_Chef/04/03__...  \n",
       "81818  data/transcripts/5480___Mind_of_a_Chef/04/04__...  \n",
       "81819  data/transcripts/5480___Mind_of_a_Chef/04/05__...  \n",
       "81820  data/transcripts/5480___Mind_of_a_Chef/04/06__...  \n",
       "81821  data/transcripts/5480___Mind_of_a_Chef/04/07__...  \n",
       "\n",
       "[81822 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paths_episodes = import_data.get_df_paths_episodes()\n",
    "df_paths_episodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On tokenize et lemmatize nos épisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/baptiste/Documents/github/courses/movies-recommendation-and-subtitles-analysis/prepare_data2.ipynb Cellule 5\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/baptiste/Documents/github/courses/movies-recommendation-and-subtitles-analysis/prepare_data2.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#nlt.tokenize_and_lemmatize_df_path_episodes_and_save(df_paths_episodes)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/baptiste/Documents/github/courses/movies-recommendation-and-subtitles-analysis/prepare_data2.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_paths_episodes[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mswifter\u001b[39m.\u001b[39;49mapply(nlt\u001b[39m.\u001b[39;49mtokenize_lemmatize_transcript_and_save)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/swifter/swifter.py:296\u001b[0m, in \u001b[0;36mSeriesAccessor.apply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39m# If parallel processing is forced by the user, then skip the logic and apply dask\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_force_parallel:\n\u001b[0;32m--> 296\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dask_apply(func, convert_dtype, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    298\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj\u001b[39m.\u001b[39miloc[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_SAMPLE_INDEX]\n\u001b[1;32m    299\u001b[0m \u001b[39m# check if input is string or\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m# if the user is overriding the string processing default\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/swifter/swifter.py:234\u001b[0m, in \u001b[0;36mSeriesAccessor._dask_apply\u001b[0;34m(self, func, convert_dtype, *args, **kwds)\u001b[0m\n\u001b[1;32m    232\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj\u001b[39m.\u001b[39miloc[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_SAMPLE_INDEX]\n\u001b[1;32m    233\u001b[0m \u001b[39mwith\u001b[39;00m suppress_stdout_stderr_logging():\n\u001b[0;32m--> 234\u001b[0m     meta \u001b[39m=\u001b[39m sample\u001b[39m.\u001b[39;49mapply(func, convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype, args\u001b[39m=\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[39m# check that the dask map partitions matches the pandas apply\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[39mwith\u001b[39;00m suppress_stdout_stderr_logging():\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1157\u001b[0m             values,\n\u001b[1;32m   1158\u001b[0m             f,\n\u001b[1;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/github/courses/movies-recommendation-and-subtitles-analysis/modules/natural_language_treatment.py:32\u001b[0m, in \u001b[0;36mtokenize_lemmatize_transcript_and_save\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_lemmatize_transcript_and_save\u001b[39m(path):\n\u001b[0;32m---> 32\u001b[0m   lematized \u001b[39m=\u001b[39m get_lemmatized_transcript_from_path(path)\n\u001b[1;32m     33\u001b[0m   lower_case_lematized \u001b[39m=\u001b[39m [word\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m lematized]\n\u001b[1;32m     34\u001b[0m   \u001b[39mdel\u001b[39;00m lematized\n",
      "File \u001b[0;32m~/Documents/github/courses/movies-recommendation-and-subtitles-analysis/modules/natural_language_treatment.py:23\u001b[0m, in \u001b[0;36mget_lemmatized_transcript_from_path\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     21\u001b[0m transcript \u001b[39m=\u001b[39m load_transcript_from_path(path)\n\u001b[1;32m     22\u001b[0m transcript_tokenized \u001b[39m=\u001b[39m  prepare_data\u001b[39m.\u001b[39mprepare_data(transcript)\n\u001b[0;32m---> 23\u001b[0m \u001b[39mreturn\u001b[39;00m lemmatize_sentence(transcript_tokenized)\n",
      "File \u001b[0;32m~/Documents/github/courses/movies-recommendation-and-subtitles-analysis/modules/natural_language_treatment.py:12\u001b[0m, in \u001b[0;36mlemmatize_sentence\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize_sentence\u001b[39m(sentence):\n\u001b[0;32m---> 12\u001b[0m   nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39men_core_web_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m, disable\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mparser\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mner\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtagger\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     13\u001b[0m   \u001b[39mreturn\u001b[39;00m [word\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m nlp(sentence)]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     39\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     55\u001b[0m         name,\n\u001b[1;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/util.py:442\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m get_lang_class(name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mblank:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))()\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m is_package(name):  \u001b[39m# installed as package\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_package(name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m Path(name)\u001b[39m.\u001b[39mexists():  \u001b[39m# path to model data directory\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/util.py:478\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \n\u001b[1;32m    463\u001b[0m \u001b[39mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[39mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 478\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload(vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, enable\u001b[39m=\u001b[39;49menable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/en_core_web_sm/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_init_py(\u001b[39m__file__\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moverrides)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/util.py:659\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_path\u001b[39m.\u001b[39mexists():\n\u001b[1;32m    658\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE052\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mdata_path))\n\u001b[0;32m--> 659\u001b[0m \u001b[39mreturn\u001b[39;00m load_model_from_path(\n\u001b[1;32m    660\u001b[0m     data_path,\n\u001b[1;32m    661\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m    662\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[1;32m    663\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m    664\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m    665\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m    666\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    667\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/util.py:524\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    515\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39moverrides)\n\u001b[1;32m    516\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(\n\u001b[1;32m    517\u001b[0m     config,\n\u001b[1;32m    518\u001b[0m     vocab\u001b[39m=\u001b[39mvocab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m     meta\u001b[39m=\u001b[39mmeta,\n\u001b[1;32m    523\u001b[0m )\n\u001b[0;32m--> 524\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39;49mfrom_disk(model_path, exclude\u001b[39m=\u001b[39;49mexclude, overrides\u001b[39m=\u001b[39;49moverrides)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/language.py:2125\u001b[0m, in \u001b[0;36mLanguage.from_disk\u001b[0;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (path \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mexists() \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude:  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m   2123\u001b[0m     \u001b[39m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[1;32m   2124\u001b[0m     exclude \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(exclude) \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m-> 2125\u001b[0m util\u001b[39m.\u001b[39;49mfrom_disk(path, deserializers, exclude)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m path  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_link_components()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/util.py:1369\u001b[0m, in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[39mfor\u001b[39;00m key, reader \u001b[39min\u001b[39;00m readers\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1367\u001b[0m     \u001b[39m# Split to support file names like meta.json\u001b[39;00m\n\u001b[1;32m   1368\u001b[0m     \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude:\n\u001b[0;32m-> 1369\u001b[0m         reader(path \u001b[39m/\u001b[39;49m key)\n\u001b[1;32m   1370\u001b[0m \u001b[39mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/language.py:2101\u001b[0m, in \u001b[0;36mLanguage.from_disk.<locals>.deserialize_vocab\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m   2099\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeserialize_vocab\u001b[39m(path: Path) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mexists():\n\u001b[0;32m-> 2101\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mfrom_disk(path, exclude\u001b[39m=\u001b[39;49mexclude)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/vocab.pyx:494\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.from_disk\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/lookups.py:313\u001b[0m, in \u001b[0;36mLookups.from_disk\u001b[0;34m(self, path, filename, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[39mwith\u001b[39;00m filepath\u001b[39m.\u001b[39mopen(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file_:\n\u001b[1;32m    312\u001b[0m         data \u001b[39m=\u001b[39m file_\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_bytes(data)\n\u001b[1;32m    314\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/lookups.py:277\u001b[0m, in \u001b[0;36mLookups.from_bytes\u001b[0;34m(self, bytes_data, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tables \u001b[39m=\u001b[39m {}\n\u001b[1;32m    276\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m srsly\u001b[39m.\u001b[39mmsgpack_loads(bytes_data)\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tables[key] \u001b[39m=\u001b[39m Table(key, value)\n\u001b[1;32m    278\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/lookups.py:79\u001b[0m, in \u001b[0;36mTable.__init__\u001b[0;34m(self, name, data)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbloom \u001b[39m=\u001b[39m BloomFilter\u001b[39m.\u001b[39mfrom_error_rate(size)\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(data)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/spacy/lookups.py:81\u001b[0m, in \u001b[0;36mTable.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m     79\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(data)\n\u001b[0;32m---> 81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__setitem__\u001b[39m(\u001b[39mself\u001b[39m, key: Union[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m], value: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     \u001b[39m\"\"\"Set new key/value pair. String keys will be hashed.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[39m    key (str / int): The key to set.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39m    value: The value to set.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     key \u001b[39m=\u001b[39m get_string_id(key)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#nlt.tokenize_and_lemmatize_df_path_episodes_and_save(df_paths_episodes)\n",
    "df_paths_episodes['path'].swifter.apply(nlt.tokenize_lemmatize_transcript_and_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serie</th>\n",
       "      <th>season</th>\n",
       "      <th>episode_number</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>path</th>\n",
       "      <th>path_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot_(1)</td>\n",
       "      <td>data/transcripts/1___Lost/01/01__Pilot_(1).txt</td>\n",
       "      <td>data/lemmatized/1___Lost/01/01__Pilot_(1).txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>2</td>\n",
       "      <td>Pilot_(2)</td>\n",
       "      <td>data/transcripts/1___Lost/01/02__Pilot_(2).txt</td>\n",
       "      <td>data/lemmatized/1___Lost/01/02__Pilot_(2).txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>3</td>\n",
       "      <td>Tabula_rasa</td>\n",
       "      <td>data/transcripts/1___Lost/01/03__Tabula_rasa.txt</td>\n",
       "      <td>data/lemmatized/1___Lost/01/03__Tabula_rasa.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>4</td>\n",
       "      <td>Walkabout</td>\n",
       "      <td>data/transcripts/1___Lost/01/04__Walkabout.txt</td>\n",
       "      <td>data/lemmatized/1___Lost/01/04__Walkabout.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lost</td>\n",
       "      <td>01</td>\n",
       "      <td>5</td>\n",
       "      <td>White_rabbit</td>\n",
       "      <td>data/transcripts/1___Lost/01/05__White_rabbit.txt</td>\n",
       "      <td>data/lemmatized/1___Lost/01/05__White_rabbit.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81817</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>3</td>\n",
       "      <td>Rome</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/03__...</td>\n",
       "      <td>data/lemmatized/5480___Mind_of_a_Chef/04/03__R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81818</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>4</td>\n",
       "      <td>Hunger</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/04__...</td>\n",
       "      <td>data/lemmatized/5480___Mind_of_a_Chef/04/04__H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81819</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>5</td>\n",
       "      <td>Past</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/05__...</td>\n",
       "      <td>data/lemmatized/5480___Mind_of_a_Chef/04/05__P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81820</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>6</td>\n",
       "      <td>Hustle</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/06__...</td>\n",
       "      <td>data/lemmatized/5480___Mind_of_a_Chef/04/06__H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81821</th>\n",
       "      <td>Mind_of_a_Chef</td>\n",
       "      <td>04</td>\n",
       "      <td>7</td>\n",
       "      <td>Napkin</td>\n",
       "      <td>data/transcripts/5480___Mind_of_a_Chef/04/07__...</td>\n",
       "      <td>data/lemmatized/5480___Mind_of_a_Chef/04/07__N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81822 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                serie season  episode_number  episode_name  \\\n",
       "0                Lost     01               1     Pilot_(1)   \n",
       "1                Lost     01               2     Pilot_(2)   \n",
       "2                Lost     01               3   Tabula_rasa   \n",
       "3                Lost     01               4     Walkabout   \n",
       "4                Lost     01               5  White_rabbit   \n",
       "...               ...    ...             ...           ...   \n",
       "81817  Mind_of_a_Chef     04               3          Rome   \n",
       "81818  Mind_of_a_Chef     04               4        Hunger   \n",
       "81819  Mind_of_a_Chef     04               5          Past   \n",
       "81820  Mind_of_a_Chef     04               6        Hustle   \n",
       "81821  Mind_of_a_Chef     04               7        Napkin   \n",
       "\n",
       "                                                    path  \\\n",
       "0         data/transcripts/1___Lost/01/01__Pilot_(1).txt   \n",
       "1         data/transcripts/1___Lost/01/02__Pilot_(2).txt   \n",
       "2       data/transcripts/1___Lost/01/03__Tabula_rasa.txt   \n",
       "3         data/transcripts/1___Lost/01/04__Walkabout.txt   \n",
       "4      data/transcripts/1___Lost/01/05__White_rabbit.txt   \n",
       "...                                                  ...   \n",
       "81817  data/transcripts/5480___Mind_of_a_Chef/04/03__...   \n",
       "81818  data/transcripts/5480___Mind_of_a_Chef/04/04__...   \n",
       "81819  data/transcripts/5480___Mind_of_a_Chef/04/05__...   \n",
       "81820  data/transcripts/5480___Mind_of_a_Chef/04/06__...   \n",
       "81821  data/transcripts/5480___Mind_of_a_Chef/04/07__...   \n",
       "\n",
       "                                         path_lemmatized  \n",
       "0          data/lemmatized/1___Lost/01/01__Pilot_(1).txt  \n",
       "1          data/lemmatized/1___Lost/01/02__Pilot_(2).txt  \n",
       "2        data/lemmatized/1___Lost/01/03__Tabula_rasa.txt  \n",
       "3          data/lemmatized/1___Lost/01/04__Walkabout.txt  \n",
       "4       data/lemmatized/1___Lost/01/05__White_rabbit.txt  \n",
       "...                                                  ...  \n",
       "81817  data/lemmatized/5480___Mind_of_a_Chef/04/03__R...  \n",
       "81818  data/lemmatized/5480___Mind_of_a_Chef/04/04__H...  \n",
       "81819  data/lemmatized/5480___Mind_of_a_Chef/04/05__P...  \n",
       "81820  data/lemmatized/5480___Mind_of_a_Chef/04/06__H...  \n",
       "81821  data/lemmatized/5480___Mind_of_a_Chef/04/07__N...  \n",
       "\n",
       "[81822 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paths_episodes['path_lemmatized'] = df_paths_episodes['path'].swifter.apply(lambda x: x.replace('data/transcripts', 'data/lemmatized'))\n",
    "df_paths_episodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant chercher à déterminer le vocabulaire de notre corpus, pour cela on va utiliser la fonction CountVectorizer de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
